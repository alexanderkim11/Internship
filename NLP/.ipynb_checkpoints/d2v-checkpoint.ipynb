{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Import Packages:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Custom Tokenizer:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell downloads the built-in stop words from NLTK and provides a function to split, process, and tokenize text.\n",
    "Processing includes stemming each word using NLTK's SnowballStemmer and removing punctuation using a regex.\n",
    "\"\"\"\n",
    "\n",
    "#Download NLTK's english stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords1 = stopwords.words(\"english\")\n",
    "\n",
    "#Regex to remove punctuation and NLTK SnowballStemmer\n",
    "#TODO: RE_PUNCT isn't great at handling when punctuation actually should be included.  Needs to be replaced with something\n",
    "#      more robust\n",
    "RE_PUNCT = re.compile('([%s])+' % re.escape(string.punctuation), re.UNICODE)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function Name:  preprocess()\n",
    "\n",
    "Parameters:     \n",
    "    text = Some string/collection of strings\n",
    "    \n",
    "Example:  preprocess(\"This is a string\")\n",
    "\n",
    "Output: List containing all of the processed/stemmed words, Type: List\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Add section to handle malformed input\n",
    "def preprocess(text):\n",
    "    res = []\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        if token not in stopwords1 and len(token) > 3 and token != \"top.location.href=location.href\":\n",
    "            token = RE_PUNCT.sub(\" \", token)\n",
    "            token = stemmer.stem(token)\n",
    "            res.append(token)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING: This cell requires a folder named scripts in the current directory in order to function properly\n",
    "\n",
    "This section loops through all files in the specified directory, passes all the text in each file through the preprocess()\n",
    "function and then loads it into the text corpus.  It also creates another list called \"files\" that contains the all of the\n",
    "filenames\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Add input section for filepath to handle any folder location \n",
    "\n",
    "#scripts = [x[2] for x in os.walk(str(os.getcwd()) + \"\\\\scripts\")]\n",
    "os.chdir(\"scripts\")\n",
    "try:\n",
    "    corpus = []\n",
    "    files = []\n",
    "    for filename in glob.glob(\"*.txt\"):\n",
    "        file = open(filename, 'r')\n",
    "        tokens = preprocess(file.read())\n",
    "        text = ' '.join(tokens)\n",
    "        corpus.append(text)\n",
    "        files.append(filename)\n",
    "        file.close()\n",
    "    os.chdir(\"..\")\n",
    "except:\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Things-I-Hate-About-You.txt</td>\n",
       "      <td>written karen mccullah lutz  amp  kirsten smit...</td>\n",
       "      <td>written karen mccullah lutz amp kirsten smith ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-and-Holding.txt</td>\n",
       "      <td>written anthoni cipriano twin boys  rudi jacob...</td>\n",
       "      <td>written anthoni cipriano twin boys rudi jacob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12-Monkeys.txt</td>\n",
       "      <td>origin screenplay david peopl janet peopl insp...</td>\n",
       "      <td>origin screenplay david peopl janet peopl insp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-Years-a-Slave.txt</td>\n",
       "      <td>written john ridley close pair black hand open...</td>\n",
       "      <td>written john ridley close pair black hand open...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127-Hours.txt</td>\n",
       "      <td>written simon beaufoy  amp  danni boyl massiv ...</td>\n",
       "      <td>written simon beaufoy amp danni boyl massiv cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1492-Conquest-of-Paradise.txt</td>\n",
       "      <td>roselyn bosch septemb 1991 start man  eleg sli...</td>\n",
       "      <td>roselyn bosch septemb      start man eleg slip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15-Minutes.txt</td>\n",
       "      <td>word czech airline  pan across word side plane...</td>\n",
       "      <td>word czech airline pan across word side plane ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17-Again.txt</td>\n",
       "      <td>written jason filardi octob 2007 car scatter p...</td>\n",
       "      <td>written jason filardi octob      car scatter p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>187.txt</td>\n",
       "      <td>scott yagemann revis shoot draft novemb 1996 e...</td>\n",
       "      <td>scott yagemann revis shoot draft novemb      e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001-A-Space-Odyssey.txt</td>\n",
       "      <td>screenplay stanley kubrick arthur clark hawk f...</td>\n",
       "      <td>screenplay stanley kubrick arthur clark hawk f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0  10-Things-I-Hate-About-You.txt   \n",
       "1              12-and-Holding.txt   \n",
       "2                  12-Monkeys.txt   \n",
       "3            12-Years-a-Slave.txt   \n",
       "4                   127-Hours.txt   \n",
       "5   1492-Conquest-of-Paradise.txt   \n",
       "6                  15-Minutes.txt   \n",
       "7                    17-Again.txt   \n",
       "8                         187.txt   \n",
       "9        2001-A-Space-Odyssey.txt   \n",
       "\n",
       "                                           documents  \\\n",
       "0  written karen mccullah lutz  amp  kirsten smit...   \n",
       "1  written anthoni cipriano twin boys  rudi jacob...   \n",
       "2  origin screenplay david peopl janet peopl insp...   \n",
       "3  written john ridley close pair black hand open...   \n",
       "4  written simon beaufoy  amp  danni boyl massiv ...   \n",
       "5  roselyn bosch septemb 1991 start man  eleg sli...   \n",
       "6  word czech airline  pan across word side plane...   \n",
       "7  written jason filardi octob 2007 car scatter p...   \n",
       "8  scott yagemann revis shoot draft novemb 1996 e...   \n",
       "9  screenplay stanley kubrick arthur clark hawk f...   \n",
       "\n",
       "                                   documents_cleaned  \n",
       "0  written karen mccullah lutz amp kirsten smith ...  \n",
       "1  written anthoni cipriano twin boys rudi jacob ...  \n",
       "2  origin screenplay david peopl janet peopl insp...  \n",
       "3  written john ridley close pair black hand open...  \n",
       "4  written simon beaufoy amp danni boyl massiv cr...  \n",
       "5  roselyn bosch septemb      start man eleg slip...  \n",
       "6  word czech airline pan across word side plane ...  \n",
       "7  written jason filardi octob      car scatter p...  \n",
       "8  scott yagemann revis shoot draft novemb      e...  \n",
       "9  screenplay stanley kubrick arthur clark hawk f...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the processed text into a Pandas DataFrame\n",
    "\"\"\"\n",
    "\n",
    "documents_df = pd.DataFrame(files, columns = ['filename'])\n",
    "documents_df[\"documents\"] = corpus\n",
    "\n",
    "#TODO: The documents_cleaned column may be redundant, possibly remove\n",
    "documents_df[\"documents_cleaned\"] = documents_df.documents.apply(lambda x: \" \".join(re.sub(r'[^a-zA-Z]',' ', w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ', w).lower() not in stopwords1))\n",
    "documents_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Doc2Vec Build and Training:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Much of the Doc2Vec code is sourced from the article referenced at the bottom of this notebook.  \n",
    "This cell downloads NLTK's built-in tokenizer punkt and applies it to every document in documents_cleaned\n",
    "\n",
    "TODO: Gain better understanding of hyperparameters for Doc2Vec (vector_size, alpha, min_count, etc.) and \n",
    "      possibly adjust to optimize model\n",
    "\"\"\"\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "#TODO:  Again, this tokenizer section may be redundant, possibly remove\n",
    "tagged_data = [TaggedDocument(words=word_tokenize(doc), tags=[i]) for i, doc in enumerate(documents_df.documents_cleaned)]\n",
    "\n",
    "\n",
    "model_d2v = Doc2Vec(vector_size=100,alpha=0.025, min_count=1)\n",
    "model_d2v.build_vocab(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WARNING:  This cell may take a significant amount of time to run!  Depends on the number of epochs you specify and the specs of\n",
    "          your machine\n",
    "          \n",
    "This section sets how many epochs the D2V model is trained for and then trains the model for that many epochs.\n",
    "\"\"\"\n",
    "\n",
    "epochs = input(\"Provide number of training epochs: \")\n",
    "## 1 epoch takes about 4 minutes on my machine\n",
    "for epoch in range(10):\n",
    "    model_d2v.train(tagged_data,\n",
    "                total_examples=model_d2v.corpus_count,\n",
    "                epochs=model_d2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akim\\Anaconda3\\envs\\dst_3.6\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell stores all of the document embeddings calculated from the doc2vec model into a list\n",
    "\n",
    "\"\"\"\n",
    "document_embeddings=np.zeros((documents_df.shape[0],100))\n",
    "for i in range(len(document_embeddings)):\n",
    "    document_embeddings[i]=model_d2v.docvecs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Similarity Calculation:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function displays the 5 most similar documents to a provided document based on a specified metric (cosine similarity,\n",
    "or euclidean distance).\n",
    "\n",
    "Function: most_similar(doc_id,similarity_matrix,matrix)\n",
    "\n",
    "Parameters:\n",
    "    doc_id = Index of document in \"documents_df\" DataFrame.  See Miscellaneous section at the bottom or search function \n",
    "             in User Input for more info on how to find doc_id\n",
    "             \n",
    "    similarity_matrix = Either pairwise_similarities (cosine similarity) or pairwise_differences(euclidean distance)\n",
    "    \n",
    "    matrix = Either \"Cosine Similarity\" as string, or \"Euclidean Distance\" as string\n",
    "    \n",
    "   \n",
    "Example: most_similar(11,pairwise_similarities,'Cosine Similarity')\n",
    "\n",
    "Output: Prints to console the document title followed by the title and similarity score of the 5 most similar documents\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#TODO: Add input section to specify how many similar documents the user wants to display\n",
    "#TODO: Add section to handle malformed input\n",
    "def most_similar(doc_id,similarity_matrix,matrix):\n",
    "    print ('\\n')\n",
    "    print (f'Document: {documents_df.iloc[doc_id][\"filename\"]}')\n",
    "    print ('\\n')\n",
    "    print ('Similar Documents:')\n",
    "    print ('\\n')\n",
    "    if matrix=='Cosine Similarity':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n",
    "    elif matrix=='Euclidean Distance':\n",
    "        similar_ix=np.argsort(similarity_matrix[doc_id])\n",
    "    for ix in similar_ix[:6]:\n",
    "        if ix==doc_id:\n",
    "            continue\n",
    "        print (f'Document: {documents_df.iloc[ix][\"filename\"]}')\n",
    "        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the cosine similarities and euclidean distances for the vectors in document_embeddings\n",
    "\"\"\"\n",
    "\n",
    "pairwise_similarities=cosine_similarity(document_embeddings)\n",
    "pairwise_differences=euclidean_distances(document_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>User Input:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function takes in a document title as a string and returns its index in documents_df\n",
    "\n",
    "Function: search(title)\n",
    "\n",
    "\n",
    "Parameters:\n",
    "    title = Document title as a string\n",
    "    \n",
    "Example: search(\"The Avengers\")\n",
    "\n",
    "\n",
    "Output: Index of the specified document if found, Type: Int. None if not found\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def search(title):\n",
    "    word = title\n",
    "    tokens = word.split()\n",
    "    res = []\n",
    "    for token in tokens:\n",
    "        token = RE_PUNCT.sub(\"\", token)\n",
    "        res.append(token)\n",
    "    boole = False\n",
    "    title = \"\"\n",
    "    for i in range(len(res)):\n",
    "        if (res[i] == \"The\") and i == 0:\n",
    "            boole = True\n",
    "            continue\n",
    "        cur = res[i]\n",
    "        if i == len(res) -  1:\n",
    "            if boole == True:\n",
    "                title += cur + \",-The.txt\"\n",
    "            else:\n",
    "                title += cur + \".txt\"\n",
    "        else:\n",
    "            title += cur + \"-\"\n",
    "    try:\n",
    "        print(title)\n",
    "        return documents_df[documents_df[\"filename\"] == title].index.values.astype(int)[0]\n",
    "    except:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next.txt\n",
      "\n",
      "\n",
      "Document: Next.txt\n",
      "\n",
      "\n",
      "Similar Documents:\n",
      "\n",
      "\n",
      "Document: TRON-Legacy.txt\n",
      "Cosine Similarity : 0.4917086951760241\n",
      "\n",
      "\n",
      "Document: Friday-the-13th.txt\n",
      "Cosine Similarity : 0.4734951137488042\n",
      "\n",
      "\n",
      "Document: Assassins.txt\n",
      "Cosine Similarity : 0.4436002238968801\n",
      "\n",
      "\n",
      "Document: Broken-Arrow.txt\n",
      "Cosine Similarity : 0.44222796752546045\n",
      "\n",
      "\n",
      "Document: Ninja-Assassin.txt\n",
      "Cosine Similarity : 0.4271594428864344\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Document: Next.txt\n",
      "\n",
      "\n",
      "Similar Documents:\n",
      "\n",
      "\n",
      "Document: Ninja-Assassin.txt\n",
      "Euclidean Distance : 19.98005259104115\n",
      "\n",
      "\n",
      "Document: Assassins.txt\n",
      "Euclidean Distance : 20.349210261699078\n",
      "\n",
      "\n",
      "Document: Long-Kiss-Goodnight,-The.txt\n",
      "Euclidean Distance : 20.453097332835984\n",
      "\n",
      "\n",
      "Document: Hellboy-2-The-Golden-Army.txt\n",
      "Euclidean Distance : 20.65944038413399\n",
      "\n",
      "\n",
      "Document: Broken-Arrow.txt\n",
      "Euclidean Distance : 20.93041360762031\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section requests a document name as input, then prints out the 5 most similar documents based on cosine similarity\n",
    "and euclidean distance.\n",
    "\n",
    "NOTE: These may not be the same lists of documents!\n",
    "\"\"\"\n",
    "\n",
    "term = input(\"Provide a Movie Title: \")\n",
    "index = search(term)\n",
    "try:\n",
    "    most_similar(index,pairwise_similarities,'Cosine Similarity')\n",
    "    print(\"-------------------------------------------------------------------------------------------\")\n",
    "    most_similar(index,pairwise_differences,'Euclidean Distance')\n",
    "except:\n",
    "    print(\"Movie not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Export  to CSV Files:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Things-I-Hate-About-You.txt</td>\n",
       "      <td>-1.695494</td>\n",
       "      <td>-5.226159</td>\n",
       "      <td>-0.107940</td>\n",
       "      <td>-2.501784</td>\n",
       "      <td>2.781953</td>\n",
       "      <td>-0.499324</td>\n",
       "      <td>-2.424419</td>\n",
       "      <td>0.260161</td>\n",
       "      <td>-0.041970</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.985297</td>\n",
       "      <td>0.162471</td>\n",
       "      <td>-3.304616</td>\n",
       "      <td>-0.091254</td>\n",
       "      <td>0.272466</td>\n",
       "      <td>0.801572</td>\n",
       "      <td>0.166671</td>\n",
       "      <td>-0.190785</td>\n",
       "      <td>2.598757</td>\n",
       "      <td>2.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-and-Holding.txt</td>\n",
       "      <td>-3.933124</td>\n",
       "      <td>2.495834</td>\n",
       "      <td>1.707793</td>\n",
       "      <td>-0.178698</td>\n",
       "      <td>1.002381</td>\n",
       "      <td>-1.182322</td>\n",
       "      <td>-1.213156</td>\n",
       "      <td>-2.730105</td>\n",
       "      <td>0.773482</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126593</td>\n",
       "      <td>3.623724</td>\n",
       "      <td>-3.633179</td>\n",
       "      <td>-2.412675</td>\n",
       "      <td>0.629609</td>\n",
       "      <td>-1.278768</td>\n",
       "      <td>1.284717</td>\n",
       "      <td>-0.511213</td>\n",
       "      <td>1.908127</td>\n",
       "      <td>-2.933471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12-Monkeys.txt</td>\n",
       "      <td>0.979229</td>\n",
       "      <td>-2.909092</td>\n",
       "      <td>1.554276</td>\n",
       "      <td>-1.672176</td>\n",
       "      <td>2.043409</td>\n",
       "      <td>-0.228946</td>\n",
       "      <td>1.858837</td>\n",
       "      <td>1.399809</td>\n",
       "      <td>0.539318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.941755</td>\n",
       "      <td>-0.401359</td>\n",
       "      <td>-5.139955</td>\n",
       "      <td>-1.320575</td>\n",
       "      <td>3.104685</td>\n",
       "      <td>1.147276</td>\n",
       "      <td>0.799477</td>\n",
       "      <td>-0.791513</td>\n",
       "      <td>3.614176</td>\n",
       "      <td>-4.387134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12-Years-a-Slave.txt</td>\n",
       "      <td>-1.986064</td>\n",
       "      <td>-0.311113</td>\n",
       "      <td>-0.578370</td>\n",
       "      <td>-0.629918</td>\n",
       "      <td>0.316444</td>\n",
       "      <td>-2.647151</td>\n",
       "      <td>-0.693074</td>\n",
       "      <td>-3.035693</td>\n",
       "      <td>-0.448140</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.240939</td>\n",
       "      <td>-4.537358</td>\n",
       "      <td>-1.122413</td>\n",
       "      <td>4.196093</td>\n",
       "      <td>-0.300379</td>\n",
       "      <td>1.185855</td>\n",
       "      <td>-3.441469</td>\n",
       "      <td>3.037497</td>\n",
       "      <td>3.211800</td>\n",
       "      <td>-4.413497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127-Hours.txt</td>\n",
       "      <td>-0.021674</td>\n",
       "      <td>-3.155433</td>\n",
       "      <td>-1.103246</td>\n",
       "      <td>1.474490</td>\n",
       "      <td>-0.407338</td>\n",
       "      <td>0.853136</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>-1.332900</td>\n",
       "      <td>-2.341174</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.763430</td>\n",
       "      <td>0.074342</td>\n",
       "      <td>-3.135787</td>\n",
       "      <td>1.149290</td>\n",
       "      <td>-0.963436</td>\n",
       "      <td>1.913585</td>\n",
       "      <td>-0.457561</td>\n",
       "      <td>3.311870</td>\n",
       "      <td>0.910334</td>\n",
       "      <td>-1.389566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>You-Can-Count-On-Me.txt</td>\n",
       "      <td>-2.743547</td>\n",
       "      <td>1.553251</td>\n",
       "      <td>-0.321049</td>\n",
       "      <td>1.076263</td>\n",
       "      <td>-2.052466</td>\n",
       "      <td>-2.686030</td>\n",
       "      <td>-1.510546</td>\n",
       "      <td>-0.810123</td>\n",
       "      <td>-2.441458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.637446</td>\n",
       "      <td>-2.392711</td>\n",
       "      <td>-1.840004</td>\n",
       "      <td>1.098260</td>\n",
       "      <td>1.445133</td>\n",
       "      <td>-0.826735</td>\n",
       "      <td>1.655933</td>\n",
       "      <td>-2.039146</td>\n",
       "      <td>-0.266891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>Youth-in-Revolt.txt</td>\n",
       "      <td>-1.558215</td>\n",
       "      <td>4.043688</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>-0.479457</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>1.738303</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>-0.205494</td>\n",
       "      <td>-0.611698</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.535653</td>\n",
       "      <td>4.131962</td>\n",
       "      <td>-1.593631</td>\n",
       "      <td>-2.437151</td>\n",
       "      <td>1.769728</td>\n",
       "      <td>1.533881</td>\n",
       "      <td>-3.058193</td>\n",
       "      <td>2.303478</td>\n",
       "      <td>1.352717</td>\n",
       "      <td>0.353871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>Zero-Dark-Thirty.txt</td>\n",
       "      <td>-0.604094</td>\n",
       "      <td>-5.703767</td>\n",
       "      <td>-0.686788</td>\n",
       "      <td>-2.622568</td>\n",
       "      <td>1.095446</td>\n",
       "      <td>1.297923</td>\n",
       "      <td>0.445318</td>\n",
       "      <td>-0.818631</td>\n",
       "      <td>0.537166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929634</td>\n",
       "      <td>3.025182</td>\n",
       "      <td>-3.500828</td>\n",
       "      <td>1.804808</td>\n",
       "      <td>1.331443</td>\n",
       "      <td>3.945112</td>\n",
       "      <td>-0.667277</td>\n",
       "      <td>1.800655</td>\n",
       "      <td>0.676404</td>\n",
       "      <td>0.368759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>Zerophilia.txt</td>\n",
       "      <td>-2.455432</td>\n",
       "      <td>-0.265861</td>\n",
       "      <td>3.578417</td>\n",
       "      <td>1.691035</td>\n",
       "      <td>1.314801</td>\n",
       "      <td>-1.981007</td>\n",
       "      <td>-4.344603</td>\n",
       "      <td>0.935932</td>\n",
       "      <td>-2.484688</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021298</td>\n",
       "      <td>1.165887</td>\n",
       "      <td>-1.294171</td>\n",
       "      <td>-1.052540</td>\n",
       "      <td>0.794281</td>\n",
       "      <td>3.936910</td>\n",
       "      <td>-0.500925</td>\n",
       "      <td>-0.956939</td>\n",
       "      <td>2.944508</td>\n",
       "      <td>0.212196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Zootopia.txt</td>\n",
       "      <td>-1.256273</td>\n",
       "      <td>1.176809</td>\n",
       "      <td>2.511914</td>\n",
       "      <td>-0.290346</td>\n",
       "      <td>0.637846</td>\n",
       "      <td>1.602748</td>\n",
       "      <td>1.764389</td>\n",
       "      <td>3.271803</td>\n",
       "      <td>0.389542</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.048933</td>\n",
       "      <td>5.525182</td>\n",
       "      <td>-1.551391</td>\n",
       "      <td>-2.165151</td>\n",
       "      <td>0.962559</td>\n",
       "      <td>1.418311</td>\n",
       "      <td>0.666421</td>\n",
       "      <td>0.085006</td>\n",
       "      <td>0.101822</td>\n",
       "      <td>-2.886292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>954 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename         0         1         2         3  \\\n",
       "0    10-Things-I-Hate-About-You.txt -1.695494 -5.226159 -0.107940 -2.501784   \n",
       "1                12-and-Holding.txt -3.933124  2.495834  1.707793 -0.178698   \n",
       "2                    12-Monkeys.txt  0.979229 -2.909092  1.554276 -1.672176   \n",
       "3              12-Years-a-Slave.txt -1.986064 -0.311113 -0.578370 -0.629918   \n",
       "4                     127-Hours.txt -0.021674 -3.155433 -1.103246  1.474490   \n",
       "..                              ...       ...       ...       ...       ...   \n",
       "949         You-Can-Count-On-Me.txt -2.743547  1.553251 -0.321049  1.076263   \n",
       "950             Youth-in-Revolt.txt -1.558215  4.043688  0.004355 -0.479457   \n",
       "951            Zero-Dark-Thirty.txt -0.604094 -5.703767 -0.686788 -2.622568   \n",
       "952                  Zerophilia.txt -2.455432 -0.265861  3.578417  1.691035   \n",
       "953                    Zootopia.txt -1.256273  1.176809  2.511914 -0.290346   \n",
       "\n",
       "            4         5         6         7         8  ...        90  \\\n",
       "0    2.781953 -0.499324 -2.424419  0.260161 -0.041970  ... -1.985297   \n",
       "1    1.002381 -1.182322 -1.213156 -2.730105  0.773482  ...  0.126593   \n",
       "2    2.043409 -0.228946  1.858837  1.399809  0.539318  ... -0.941755   \n",
       "3    0.316444 -2.647151 -0.693074 -3.035693 -0.448140  ... -1.240939   \n",
       "4   -0.407338  0.853136  0.367200 -1.332900 -2.341174  ... -1.763430   \n",
       "..        ...       ...       ...       ...       ...  ...       ...   \n",
       "949 -2.052466 -2.686030 -1.510546 -0.810123 -2.441458  ...  0.009750   \n",
       "950  0.214844  1.738303  0.040430 -0.205494 -0.611698  ... -1.535653   \n",
       "951  1.095446  1.297923  0.445318 -0.818631  0.537166  ...  0.929634   \n",
       "952  1.314801 -1.981007 -4.344603  0.935932 -2.484688  ... -0.021298   \n",
       "953  0.637846  1.602748  1.764389  3.271803  0.389542  ... -1.048933   \n",
       "\n",
       "           91        92        93        94        95        96        97  \\\n",
       "0    0.162471 -3.304616 -0.091254  0.272466  0.801572  0.166671 -0.190785   \n",
       "1    3.623724 -3.633179 -2.412675  0.629609 -1.278768  1.284717 -0.511213   \n",
       "2   -0.401359 -5.139955 -1.320575  3.104685  1.147276  0.799477 -0.791513   \n",
       "3   -4.537358 -1.122413  4.196093 -0.300379  1.185855 -3.441469  3.037497   \n",
       "4    0.074342 -3.135787  1.149290 -0.963436  1.913585 -0.457561  3.311870   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "949  0.637446 -2.392711 -1.840004  1.098260  1.445133 -0.826735  1.655933   \n",
       "950  4.131962 -1.593631 -2.437151  1.769728  1.533881 -3.058193  2.303478   \n",
       "951  3.025182 -3.500828  1.804808  1.331443  3.945112 -0.667277  1.800655   \n",
       "952  1.165887 -1.294171 -1.052540  0.794281  3.936910 -0.500925 -0.956939   \n",
       "953  5.525182 -1.551391 -2.165151  0.962559  1.418311  0.666421  0.085006   \n",
       "\n",
       "           98        99  \n",
       "0    2.598757  2.382200  \n",
       "1    1.908127 -2.933471  \n",
       "2    3.614176 -4.387134  \n",
       "3    3.211800 -4.413497  \n",
       "4    0.910334 -1.389566  \n",
       "..        ...       ...  \n",
       "949 -2.039146 -0.266891  \n",
       "950  1.352717  0.353871  \n",
       "951  0.676404  0.368759  \n",
       "952  2.944508  0.212196  \n",
       "953  0.101822 -2.886292  \n",
       "\n",
       "[954 rows x 101 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adds a filename column to the document_embeddings DataFrame to mainting index-document association for future use\n",
    "\"\"\"\n",
    "newdf = pd.DataFrame(document_embeddings)\n",
    "newdf[\"filename\"] = documents_df[\"filename\"]\n",
    "newdf = newdf[[c for c in newdf if c in [\"filename\"]] + [d for d in newdf if d not in [\"filename\"]]]\n",
    "newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Exports the documents_df and modified document_embeddings DataFrames to CSV files for use in other projects\n",
    "\"\"\"\n",
    "documents_df.to_csv(\"document_df.csv\",index = False)\n",
    "newdf.to_csv(\"document_embeddings.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Miscellaneous:</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>documents</th>\n",
       "      <th>documents_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10-Things-I-Hate-About-You.txt</td>\n",
       "      <td>written karen mccullah lutz  amp  kirsten smit...</td>\n",
       "      <td>written karen mccullah lutz amp kirsten smith ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0  10-Things-I-Hate-About-You.txt   \n",
       "\n",
       "                                           documents  \\\n",
       "0  written karen mccullah lutz  amp  kirsten smit...   \n",
       "\n",
       "                                   documents_cleaned  \n",
       "0  written karen mccullah lutz amp kirsten smith ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_df[documents_df[\"filename\"] == \"10-Things-I-Hate-About-You.txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1>Sources:</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/calculating-document-similarities-using-bert-and-other-models-b2c1a29c9630"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a1941883b2709250a3f9aea35aca4c9fe4a91c6d7b9f42dc68c18bae16893a92"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
